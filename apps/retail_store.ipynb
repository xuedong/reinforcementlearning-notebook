{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You own a bike store. During week $t$, the (random) demand is $D_t$ units. \n",
    "On Monday morning you may choose to command $A_t$ additional units: they are delivered immediately before the shop opens. For each week:\n",
    "\n",
    "  * Maintenance cost: $h$ per unit in stock left from the previous week (no maintenance is needed for newly commanded items)\n",
    "  * Command cost: $c$ for each unit ordered + $c_0$ per command\n",
    "  * Sales profit: $p$ per unit sold\n",
    "  * Constraint: \n",
    "    - your warehouse has a maximal capacity of $M$ unit (any additionnal bike gets stolen)\n",
    "    - you cannot sell bikes that you don't have in stock\n",
    "\n",
    "\n",
    "* State: number of bikes in stock left from the last week => state space $\\mathcal{S} = \\{0,\\dots, M\\}$\n",
    "* Action: number of bikes commanded at the beginning of the week => action space $\\mathcal{A} = \\{0, \\dots ,M\\}$ \n",
    "* Reward = balance of the week: if you command $a_t$ bikes,\n",
    "$$r_t = -c_0 \\mathbb{1}_{(a_t >0)}- c \\times a_t - h\\times s_t + p \\times \\min(D_t, s_t+a_t, M)$$\n",
    "* Transitions: you end the week with the number of bikes $$s_{t+1} = \\max\\big(0, \\min(M, s_t + a_t)  - D_t \\big)$$ \n",
    "\n",
    "Our goal is to minimize the discounted sum of rewards, starting from an initial stock $s_1$, that is to find a policy whose value is \n",
    "$$V^*(s_1) = \\max_{\\pi}\\mathbb{E}_{\\pi}\\left[\\sum_{s=1}^{\\infty} \\gamma^{s-1}r_s\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 15 # stock capacity\n",
    "h = 0.3 # maintenance cost (per unit)\n",
    "c = 0.5 # ordering cost (per unit)\n",
    "c0 = 0.3 # fix delivery cost per command\n",
    "p = 1 # selling price (per unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the demand distribution \n",
    "\n",
    "The demand is modelled as a truncated geometric distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demand distribution (truncated geometric with parameter q)\n",
    "q = 0.1\n",
    "pdem = np.array([q*(1-q)**m for m in range(M+1)])\n",
    "pdem[M] = pdem[M]+1-np.sum(pdem)\n",
    "\n",
    "print(\"the average demand is \",np.sum([m*pdem[m] for m in range(M+1)]))\n",
    "\n",
    "def SimuDemand(pdem): \n",
    "    cpdem = np.cumsum(pdem)\n",
    "    i=0\n",
    "    u = rd.random()\n",
    "    while (u >cpdem[i]):\n",
    "        i = i+1\n",
    "    return i \n",
    "\n",
    "print(\"a simulated demand is \",SimuDemand(pdem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of the MDP as a gym environment\n",
    "\n",
    "This is just to give an example on how gym works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextstate(s,a,d,M):\n",
    "    # computes the next state if the demand is d\n",
    "    return max(0,min(M,s+a) -d)\n",
    "\n",
    "def nextreward(s,a,d,M,c,c0,h,p):\n",
    "    # computes the next state if the demand is d\n",
    "    rew = -c*a - h*s + p*min(M,d,s+a)\n",
    "    if (a>0):\n",
    "        rew = rew - c0\n",
    "    return rew\n",
    "\n",
    "class StoreManagement(gym.Env):\n",
    "    \"\"\"\n",
    "    Retail Store Management environment\n",
    "    The environment defines which actions can be taken at which point and\n",
    "    when the agent receives which reward.\n",
    "    \"\"\"\n",
    "    def __init__(self,FirstState):\n",
    "        \n",
    "        # General variables defining the environment\n",
    "        self.Stock_Capacity = M\n",
    "        self.Maintenance_Cost = h\n",
    "        self.Order_Cost = c \n",
    "        self.Delivery_Cost = c0\n",
    "        self.Selling_Price = p\n",
    "        self.Demand_Distribution = pdem\n",
    "        \n",
    "        # Define the action space\n",
    "        self.action_space = gym.spaces.Discrete(self.Stock_Capacity+1)\n",
    "\n",
    "        # Define the state space (state space = observation space in this example)\n",
    "        self.observation_space = gym.spaces.Discrete(self.Stock_Capacity+1)\n",
    "\n",
    "        # current time step\n",
    "        self.curr_step = -1 # \n",
    "        \n",
    "        # initial state\n",
    "        self.state = FirstState\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        simulates a transition following action a in the current state\n",
    "        action : int\n",
    "        \"\"\"\n",
    "        self.curr_step += 1\n",
    "        # simulate the demand \n",
    "        Demand = SimuDemand(self.Demand_Distribution)\n",
    "        # compute the reward\n",
    "        reward = nextreward(self.state,action,Demand,self.Stock_Capacity,self.Order_Cost,self.Delivery_Cost,self.Maintenance_Cost,self.Selling_Price)\n",
    "        # compute the next state \n",
    "        self.state = nextstate(self.state,action,Demand,self.Stock_Capacity)\n",
    "        # return 4 elements: observation / reward / termination? (no terminal state here) / information (optional) \n",
    "        return self.state, reward, False, {}\n",
    "\n",
    "    def reset(self,InitialStock):\n",
    "        \"\"\"\n",
    "        Reset the state of the environment and returns an initial observation.\n",
    "        Returns\n",
    "        -------\n",
    "        observation (object): the initial observation of the space.\n",
    "        \"\"\"\n",
    "        self.curr_step = -1\n",
    "        self.state = InitialStock\n",
    "    \n",
    "    def _render(self, mode='human'):\n",
    "        \"\"\"optional visualization of the interaction: none here\"\"\"\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function that simulates a trajectory under a policy Pi starting from some state $s_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimulateTrajectory(T,Pi,s1):\n",
    "    '''input: T length of the interaction, Pi a policy (function), s1 initial state'''\n",
    "    Rewards = np.zeros(T+1)\n",
    "    States = np.zeros(T+1)\n",
    "    env=StoreManagement(s1)\n",
    "    for t in range(T):\n",
    "        States[t]=env.state\n",
    "        action=Pi(env.state)\n",
    "        state,rew,x,y=env.step(action)\n",
    "        Rewards[t]=rew\n",
    "    return States,Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running simulations with three simple baselines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 10 # initial stock \n",
    "gamma = 0.97 # discount factor \n",
    "\n",
    "def PiUniform(s):\n",
    "    # pick uniformly at random in {0,1,...,M-s}\n",
    "    x = rd.sample(range(M+1-s),1)\n",
    "    return s+x[0]\n",
    "\n",
    "def PiConstant(s,c=3):\n",
    "    # oder a constant number of c bikes \n",
    "    return min(c,M-s)\n",
    "\n",
    "def PiThreshold(s,m1=4,m2=10):\n",
    "    # if less than m1 bikes in stock, refill it up to m2\n",
    "    action = 0\n",
    "    if (s <=m1):\n",
    "        action = (m2-s)\n",
    "    return action\n",
    "        \n",
    "\n",
    "T = int(np.log(1/(0.001*(1-gamma)))/np.log(1/gamma)) # time needed for the value not to matter any more\n",
    "States1,Rewards1 = SimulateTrajectory(T, PiUniform,s1)\n",
    "States2,Rewards2 = SimulateTrajectory(T, PiConstant,s1)\n",
    "States3,Rewards3 = SimulateTrajectory(T, PiThreshold,s1)\n",
    "\n",
    "# without discount:\n",
    "plt.plot(np.cumsum(Rewards1),label=\"random policy\")\n",
    "plt.plot(np.cumsum(Rewards2),label=\"constant policy\")\n",
    "plt.plot(np.cumsum(Rewards3),label=\"threshold policy\")\n",
    "plt.xlabel('weeks')\n",
    "plt.ylabel('cumulated reward')\n",
    "plt.legend()\n",
    "\n",
    "# with discount:\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(Rewards1*np.array([gamma**t for t in range(T+1)])),label=\"random policy\")\n",
    "plt.plot(np.cumsum(Rewards2*np.array([gamma**t for t in range(T+1)])),label=\"constant policy\")\n",
    "plt.plot(np.cumsum(Rewards3*np.array([gamma**t for t in range(T+1)])),label=\"threshold policy\")\n",
    "plt.xlabel('weeks')\n",
    "plt.ylabel('cumulated discounted reward')\n",
    "plt.legend()\n",
    "\n",
    "#plot(p1, p2, layout = (2,1), size = (800, 800))\n",
    "#png(\"shop_run_profit.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of the stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(States1)\n",
    "plt.xlabel('weeks')\n",
    "plt.ylabel('stock')\n",
    "plt.title('Evolution of the stock under a random policy')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(States2)\n",
    "plt.xlabel('weeks')\n",
    "plt.ylabel('stock')\n",
    "plt.title('Evolution of the stock under a constant policy')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(States3)\n",
    "plt.xlabel('weeks')\n",
    "plt.ylabel('stock')\n",
    "plt.title('Evolution of the stock under a threshold policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "Here we assume that we work on a *known* MDP: that is, we know the demand distribution hence we are able to compute the parameters of the MDP: the transition kernel and the average reward\n",
    "\n",
    "## Computation of the transitions and rewards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition and Reward kernels\n",
    "K = np.zeros((M+1,M+1,M+1)) # K[s,s',a] = p(s' | s,a) \n",
    "avgR = np.zeros((M+1,M+1)) # avgR[s,a] = average reward received in state s when playing action a\n",
    "\n",
    "## computation: iterate over all possible states, actions, and possible demand values\n",
    "for a in range(M+1):\n",
    "    for s in range(M+1):\n",
    "        for d in range(M+1):\n",
    "            # next state and reward with demand d\n",
    "            ns = max(0,min(M,s+a) -d)\n",
    "            reward = -c*a - h*s+p*min(M,d,s+a)\n",
    "            if (a>0):\n",
    "                reward = reward - c0\n",
    "            K[s,ns,a] += pdem[d]\n",
    "            avgR[s,a] += pdem[d] * reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of deterministic policies\n",
    "\n",
    "First, by trying Monte-Carlo (that would also work for stochastic policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the values in each state by Monte-Carlo simulation (needs many simulations to be precise)\n",
    "\n",
    "Policy=PiThreshold\n",
    "\n",
    "MC = 300 # number of Monte-Carlo simulations\n",
    "Values = np.zeros(M+1)\n",
    "\n",
    "for i in range (M+1):\n",
    "    s1=i\n",
    "    val = 0\n",
    "    for j in range(MC):\n",
    "        States,Rewards=SimulateTrajectory(T,Policy,s1)\n",
    "        val += np.sum(Rewards*np.array([gamma**t for t in range(T+1)]))\n",
    "    Values[i] = val/MC  \n",
    "\n",
    "print(\"the estimated value in each states for the threshold policy are \")\n",
    "print(Values)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then by using the matrix inversion technique. **Complete the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluatePolicy(Pi):\n",
    "    avgRPi=np.zeros((M+1,1))\n",
    "    KPi=np.zeros((M+1,M+1))\n",
    "    for s in range(M+1):\n",
    "        KPi[s,:]=K[s,:,Pi(s)] # matrix P^\\pi\n",
    "        avgRPi[s]=avgR[s,Pi(s)] # vector r^\\pi\n",
    "    V = ## TO BE COMPLETED\n",
    "    return V.reshape(M+1,)\n",
    "\n",
    "Policy=PiThreshold\n",
    "Values = EvaluatePolicy(Policy)\n",
    "\n",
    "print(Values)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the threshold policy and the constant policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Values1 = EvaluatePolicy(PiConstant)\n",
    "Values2 = EvaluatePolicy(PiThreshold)\n",
    "\n",
    "plt.plot(Values1,label =\"Constant policy c=3\")\n",
    "plt.plot(Values2,label = \"Threshold policy m1=4, m2=10\")\n",
    "plt.xlabel('stock')\n",
    "plt.ylabel('value ')\n",
    "plt.legend()\n",
    "plt.title('Evolution of the stock under a constant policy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Optimal Policy: Value and Policy Iteration\n",
    "\n",
    "We can compare the optimal policy obtained with Value and Policy Iteration, and the runtime of the two algorithms. **For Policy Iteration, you need to complete the code below**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that performs policy improvement\n",
    "\n",
    "def Improve(V):\n",
    "    '''return Pi=greedy(V) and the value function of policy Pi'''\n",
    "    Pi = np.zeros(M+1) # improved policy \n",
    "    newV = np.zeros(M+1)\n",
    "    # compute the Q table \n",
    "    Q = np.zeros((M+1,M+1))\n",
    "    for s in range(M+1):\n",
    "        for a in range(M+1):\n",
    "            Q[s,a]=avgR[s,a]+gamma*np.sum([K[s,ns,a]*V[ns] for ns in range(M+1)])\n",
    "        # improvement (greedy policy wrt to Q)\n",
    "        pi = np.argmax(Q[s,:])\n",
    "        Pi[s]=pi\n",
    "        newV[s]=Q[s,pi]\n",
    "        Pi=Pi.astype(int)\n",
    "    return Pi,newV\n",
    "\n",
    "\n",
    "def ValueIteration(epsilon=1e-3): # epsilon = guaranteed precision\n",
    "    # initialization \n",
    "    V = np.zeros(M+1)\n",
    "    Pi = np.zeros(M+1)\n",
    "    # new value function\n",
    "    newV = 10*np.random.rand(M+1)\n",
    "    nIt = 0\n",
    "    while (np.linalg.norm(V-newV)>epsilon):\n",
    "        nIt +=1\n",
    "        V = np.copy(newV) \n",
    "        Pi,newV = Improve(V)\n",
    "        #print(\"V is \",V,\" and newV is \",newV,\"\\n\")\n",
    "    return Pi,newV,nIt\n",
    "\n",
    "\n",
    "def PolicyIteration():\n",
    "    # initalization \n",
    "    Pi = np.zeros(M+1)\n",
    "    V = np.zeros(M+1)\n",
    "    # new policy \n",
    "    newPi = np.random.randint(M+1,size=M+1) \n",
    "    nIt = 0 \n",
    "    while (np.linalg.norm(Pi-newPi)>0.1):\n",
    "        nIt +=1 \n",
    "        Pi = np.copy(newPi)\n",
    "        # evaluate and improve policy Pi \n",
    "        newPi,newV = ### TO BE COMPLETED\n",
    "    return Pi,V,nIt\n",
    "        \n",
    "start = time.time()\n",
    "Pi,V,nIt = ValueIteration()\n",
    "elapsed = time.time()-start\n",
    "\n",
    "start = time.time()\n",
    "Pi2,V2,nIt2 = PolicyIteration()\n",
    "elapsed2 = time.time()-start\n",
    "\n",
    "print(\"Value iteration yields policy\",Pi,\"and value \",V,\" in \",nIt,\" iterations and t=\",elapsed,\" seconds\\n\")\n",
    "print(\"Policy iteration yields \",Pi2,\"and value \",V2,\" in \",nIt2,\" iterations and t=\",elapsed2,\" seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Pi)\n",
    "plt.xlabel('stock')\n",
    "plt.ylabel('order under the optimal policy')\n",
    "plt.title(\"Optimal Policy\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(V)\n",
    "plt.xlabel('stock')\n",
    "plt.ylabel('value of the optimal policy')\n",
    "plt.title(\"Optimal Value\")\n",
    "\n",
    "Vstar=np.copy(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Methods\n",
    "\n",
    "## Stochastic Approximation for Policy Evaluation: TD(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD0(Pi, T):\n",
    "    V = np.random.rand(M+1) # V[s] = estimated value of each state under policy pi\n",
    "    N = np.zeros(M+1) # N[s] =number of visits to state s in the loop\n",
    "    s0 = np.random.randint(M+1)\n",
    "    env=StoreManagement(s0)\n",
    "    for t in range(T):\n",
    "        S = env.state\n",
    "        N[S] += 1\n",
    "        action = Pi(S)\n",
    "        nS,rew,x,y = env.step(action) \n",
    "        alpha = 1/((1+N[S])**0.5) \n",
    "        V[S] = (1-alpha)*V[S] + alpha * (rew + gamma*V[nS]) \n",
    "    return(V)\n",
    "\n",
    "T = 10**6\n",
    "Pi = PiConstant\n",
    "\n",
    "start=time.time()\n",
    "print(\"value computed by TD0 \",TD0(Pi, T))\n",
    "print(\"time elapsed is\",time.time()-start,\"\\n\")\n",
    "print(\"value computed by matrix inversion\",EvaluatePolicy(Pi))\n",
    "# requires a large number of iterations to converge... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(Landmarks):\n",
    "    star=time.time()\n",
    "    nbLands = len(Landmarks)\n",
    "    # random initialization of the Q table\n",
    "    Q = 10*np.random.rand(1+M, 1+M) \n",
    "    N = np.zeros((M+1,M+1)) # number of visits to state-values \n",
    "    epsilon = 0.3\n",
    "    s0 = np.random.randint(M+1)\n",
    "    env=StoreManagement(s0)\n",
    "    for k in range(nbLands-1):       \n",
    "        for t in range(Landmarks[k]+1,Landmarks[k+1]):\n",
    "            S=env.state\n",
    "            A =np.random.randint(M+1)\n",
    "            if (rd.random()>epsilon):# epsilon-greedy choice of action\n",
    "                 A = np.argmax(Q[S, :]) # greedy action           \n",
    "            N[S,A] += 1\n",
    "            nS,rew,x,y = env.step(A)\n",
    "            delta = rew + gamma * max(Q[nS, :]) - Q[S,A]\n",
    "            alpha = 1/((1+N[S,A])**0.5)\n",
    "            Q[S, A] += alpha * delta\n",
    "        # compute the greedy policy \n",
    "        Pi = np.array([np.argmax(Q[s,:]) for s in range(M+1)])\n",
    "        Pi = Pi.astype(int)\n",
    "        print(\"After T=\",Landmarks[k+1],\"iterations (\",np.floor(time.time()-start),\"seconds), the policy is\",Pi)\n",
    "    return Pi,Q\n",
    "\n",
    "Landmarks = np.array([0,10**3,10**4,10**5,10**6,10**7])\n",
    "Pi,Q = QLearning(Landmarks)\n",
    "\n",
    "# for T >= 10^7, we obtain correct policies\n",
    "# for T >= 10^8, we almost obtain the right one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
